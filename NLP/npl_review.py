# -*- coding: utf-8 -*-
"""Npl_Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y8WHAcoDNahtW0DxaSumQLVLJVUPki3M
"""

import nltk as nl
import pandas as pd
import numpy as nm

dir(nl)

Review_data = pd.read_csv("./docreview2.csv",engine="python")
Review_data

Review_data1 = pd.read_csv("./docreview4.csv",engine="python")

import nltk

Review_data.head()

type(Review_data)

Reviews_doc = Review_data.iloc[:,-1]

TOTAL_DATASETS = Review_data.append(Review_data1)

TOTAL_DATASETS

TOTAL_DATASETS.to_csv('file1.csv')

TOTAL_DATASETS.iloc[:,1]

from nltk.tokenize import sent_tokenize, word_tokenize

nltk.download()

REVIEW_ONE = TOTAL_DATASETS.iloc[4,-1]
REVIEW_ONE=REVIEW_ONE[1:-2]
nltk.download('punkt')

type(REVIEW_ONE)

print(word_tokenize(REVIEW_ONE))



pip install fasttext

import fasttext

dir(fasttext)

from sklearn.model_selection import train_test_split

TRAIN_DATASET = TOTAL_DATASETS[:500]
TRAIN_DATASET.shape

TEST_DATASET = TOTAL_DATASETS[500:]

# model = fasttext.train_supervised(input=TRAIN_DATASET)
kk=[]
# for i in range(len(TRAIN_DATASET)):
#   TRAIN_DATASET.iloc[i]
str(TRAIN_DATASET.iloc[i])
type(TRAIN_DATASET.iloc[i])

kk

import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')  # English
ft = fasttext.load_model('cc.en.300.bin')



"""NLTK"""

import nltk
nltk.download('movie_reviews')

from nltk.corpus import movie_reviews
import random

documents = [(list(movie_reviews.words(fileid)), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features

all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = nltk.NaiveBayesClassifier.train(train_set)

print(nltk.classify.accuracy(classifier, test_set))

print(word_tokenize(REVIEW_ONE))

words_data= list(test_set[30][0].keys())
result_data= list(test_set[30][0].values())

l=[]



for i in words_data:
  l.append(i[9:-1])

ll=[]
for i in result_data:
  ll.append(i)

ll

print(test_set[33][1])

l

T=0
F=0
for i in ll:
  if str(i) == 'True':
    T=T+1
  elif str(i)=='False':
    F=F+1
print(T,F)

print(test_set[35][1])